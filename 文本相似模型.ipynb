{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-31T14:04:04.230784Z",
     "start_time": "2025-01-31T14:04:04.223411Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 构造文本相似度模型",
   "id": "d32b91278a45d27c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T14:04:10.691190Z",
     "start_time": "2025-01-31T14:04:04.823495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, BertPreTrainedModel, BertModel, TrainingArguments, Trainer\n",
    "from typing import Optional\n",
    "from torch.nn import CosineSimilarity, CosineEmbeddingLoss\n",
    "import torch\n",
    "import evaluate"
   ],
   "id": "9e3a62af6b43b6fc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T14:04:10.747844Z",
     "start_time": "2025-01-31T14:04:10.699520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_parquet(\"数据/文本相似模型.parquet\")\n",
    "df"
   ],
   "id": "8d63dc5912629fec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                text1  \\\n",
       "0   Docetaxel target prodrug for preventing liver ...   \n",
       "1   Docetaxel target prodrug for preventing liver ...   \n",
       "2   Docetaxel target prodrug for preventing liver ...   \n",
       "3   Docetaxel target prodrug for preventing liver ...   \n",
       "4   Docetaxel target prodrug for preventing liver ...   \n",
       "..                                                ...   \n",
       "95  LUNG CANCER DIFFERENTIAL MARKER,An object of t...   \n",
       "96  LUNG CANCER DIFFERENTIAL MARKER,An object of t...   \n",
       "97  LUNG CANCER DIFFERENTIAL MARKER,An object of t...   \n",
       "98  LUNG CANCER DIFFERENTIAL MARKER,An object of t...   \n",
       "99  LUNG CANCER DIFFERENTIAL MARKER,An object of t...   \n",
       "\n",
       "                                                text2  label  \n",
       "0   retroviruses; retroviral protease substrate li...      1  \n",
       "1   bone morphogenetic proteins; chemoprevention; ...      0  \n",
       "2   tetrahydroisoquinoline; szyldergemajn; lurbine...      0  \n",
       "3   asialoglycoproteinreceptor-mediated uptake; di...      1  \n",
       "4   ddchaohui@sina.com; colorectal cancer; lintao4...      0  \n",
       "..                                                ...    ...  \n",
       "95  immune checkpoint inhibitors; theimmune checkp...      0  \n",
       "96  lung; kwiatkowski; balasundaram; ding l; genes...      0  \n",
       "97  peptide nucleic acids; nucleic acid recognitio...      0  \n",
       "98  hypoxia; hypoxia-inducible factor-1alpha; tumo...      0  \n",
       "99  rodent monoclonal antibodies; human antibodies...      0  \n",
       "\n",
       "[100 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Docetaxel target prodrug for preventing liver ...</td>\n",
       "      <td>retroviruses; retroviral protease substrate li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Docetaxel target prodrug for preventing liver ...</td>\n",
       "      <td>bone morphogenetic proteins; chemoprevention; ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Docetaxel target prodrug for preventing liver ...</td>\n",
       "      <td>tetrahydroisoquinoline; szyldergemajn; lurbine...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Docetaxel target prodrug for preventing liver ...</td>\n",
       "      <td>asialoglycoproteinreceptor-mediated uptake; di...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Docetaxel target prodrug for preventing liver ...</td>\n",
       "      <td>ddchaohui@sina.com; colorectal cancer; lintao4...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>LUNG CANCER DIFFERENTIAL MARKER,An object of t...</td>\n",
       "      <td>immune checkpoint inhibitors; theimmune checkp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>LUNG CANCER DIFFERENTIAL MARKER,An object of t...</td>\n",
       "      <td>lung; kwiatkowski; balasundaram; ding l; genes...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>LUNG CANCER DIFFERENTIAL MARKER,An object of t...</td>\n",
       "      <td>peptide nucleic acids; nucleic acid recognitio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>LUNG CANCER DIFFERENTIAL MARKER,An object of t...</td>\n",
       "      <td>hypoxia; hypoxia-inducible factor-1alpha; tumo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>LUNG CANCER DIFFERENTIAL MARKER,An object of t...</td>\n",
       "      <td>rodent monoclonal antibodies; human antibodies...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T02:31:16.341577Z",
     "start_time": "2025-01-08T02:31:16.143597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = Dataset.from_pandas(df).train_test_split(test_size=0.1)\n",
    "dataset"
   ],
   "id": "2905232c4e44dcdf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text1', 'text2', 'label'],\n",
       "        num_rows: 21959\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text1', 'text2', 'label'],\n",
       "        num_rows: 2440\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1、处理数据",
   "id": "f9d2e1a3eeb58ae7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T02:31:16.367633Z",
     "start_time": "2025-01-08T02:31:16.365037Z"
    }
   },
   "cell_type": "code",
   "source": "model_path = \"NeuML/pubmedbert-base-embeddings\"",
   "id": "27ec5831581ea394",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T02:31:16.879805Z",
     "start_time": "2025-01-08T02:31:16.412744Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = AutoTokenizer.from_pretrained(model_path)",
   "id": "3fb8002088b75a8e",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T02:31:16.910145Z",
     "start_time": "2025-01-08T02:31:16.905965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_func(examples):\n",
    "    sentences, labels = [], []\n",
    "\n",
    "    for sent1, sent2, label in zip(examples[\"text1\"], examples[\"text2\"], examples[\"label\"]):\n",
    "        sentences.append(sent1)\n",
    "        sentences.append(sent2)\n",
    "        labels.append(1 if label == 1 else -1)\n",
    "\n",
    "    token = tokenizer(sentences, max_length=256, truncation=True, padding=\"max_length\", return_tensors=\"np\")\n",
    "    token = {k: v.reshape(-1, 2, 256) for k, v in token.items()}\n",
    "    token[\"labels\"] = labels\n",
    "    return token"
   ],
   "id": "78d96b51b91c9e26",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T02:31:23.965828Z",
     "start_time": "2025-01-08T02:31:16.957086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataloader = dataset.map(process_func, batched=True, remove_columns=dataset[\"test\"].column_names)\n",
    "dataloader"
   ],
   "id": "71439385943db0d7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/21959 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff1cca325d4b44a8b3a134af2a53b46a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2440 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dba781e7393845338b88e0760b4f1645"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21959\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2440\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2、创建模型",
   "id": "7ac08ff222b8d8e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T02:31:23.987200Z",
     "start_time": "2025-01-08T02:31:23.981302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DualModel(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: Optional[torch.Tensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            token_type_ids: Optional[torch.Tensor] = None,\n",
    "            position_ids: Optional[torch.Tensor] = None,\n",
    "            head_mask: Optional[torch.Tensor] = None,\n",
    "            inputs_embeds: Optional[torch.Tensor] = None,\n",
    "            labels: Optional[torch.Tensor] = None,\n",
    "            output_attentions: Optional[bool] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # 分别获取句子1和句子2的输入\n",
    "        text1_input_ids, text2_input_ids = input_ids[:, 0], input_ids[:, 1]\n",
    "        text1_mask, text2_mask = attention_mask[:, 0], attention_mask[:, 1]\n",
    "        text1_type_ids, text2_type_ids = token_type_ids[:, 0], token_type_ids[:, 1]\n",
    "\n",
    "        # 句子1的获取向量表示\n",
    "        text1_outputs = self.bert(\n",
    "            text1_input_ids,\n",
    "            attention_mask=text1_mask,\n",
    "            token_type_ids=text1_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        text1_pooled_output = text1_outputs[1]\n",
    "\n",
    "        # 句子2的获取向量表示\n",
    "        text2_outputs = self.bert(\n",
    "            text2_input_ids,\n",
    "            attention_mask=text2_mask,\n",
    "            token_type_ids=text2_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        text2_pooled_output = text2_outputs[1]\n",
    "\n",
    "        # 计算相似度\n",
    "        cos = CosineSimilarity()(text1_pooled_output, text2_pooled_output)\n",
    "\n",
    "        # 计算loss\n",
    "        if labels is not None:\n",
    "            loss_fct = CosineEmbeddingLoss(margin=0.3)\n",
    "            loss = loss_fct(text1_pooled_output, text2_pooled_output, labels)\n",
    "            return loss, cos\n",
    "\n",
    "        return None, cos"
   ],
   "id": "a930b14839592d3e",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T02:31:24.907604Z",
     "start_time": "2025-01-08T02:31:24.055068Z"
    }
   },
   "cell_type": "code",
   "source": "model = DualModel.from_pretrained(model_path)",
   "id": "38aef378d21ddda0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3、创建评估函数",
   "id": "1480eaff93c39064"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T02:31:33.502159Z",
     "start_time": "2025-01-08T02:31:24.916048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metric = evaluate.combine([\n",
    "    evaluate.load(\"accuracy\", average=\"macro\"),\n",
    "    evaluate.load(\"f1\", average=\"macro\"),\n",
    "    evaluate.load(\"precision\", average=\"macro\"),\n",
    "    evaluate.load(\"recall\", average=\"macro\"),\n",
    "])"
   ],
   "id": "48b73b090e8b1087",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T02:31:33.528052Z",
     "start_time": "2025-01-08T02:31:33.524546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def metric_fn(values):\n",
    "    predictions, labels = values\n",
    "    predictions = [int(p > 0.6) for p in predictions]\n",
    "    labels = [int(label > 0) for label in labels]\n",
    "    return metric.compute(predictions, labels)"
   ],
   "id": "282127c2659646c3",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4、创建训练参数",
   "id": "aa1cefcdeeb12a52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T02:31:33.645290Z",
     "start_time": "2025-01-08T02:31:33.573827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"模型/文本相似模型\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    eval_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    save_steps=100,\n",
    ")"
   ],
   "id": "1cbdf370271901c7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lixiaoying/anaconda3/envs/AiMedJupyter/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5、创建训练器",
   "id": "1ba8fab823934a19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T02:31:33.855002Z",
     "start_time": "2025-01-08T02:31:33.678982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    compute_metrics=metric_fn,\n",
    "    train_dataset=dataloader[\"train\"],\n",
    "    eval_dataset=dataloader[\"test\"],\n",
    ")"
   ],
   "id": "8d8fb7c85df25f09",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6、训练",
   "id": "1ad73fcf3dfb59bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T02:58:27.260490Z",
     "start_time": "2025-01-08T02:31:33.862851Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "ab39a34b73216d40",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2061' max='2061' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2061/2061 26:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.234600</td>\n",
       "      <td>0.196976</td>\n",
       "      <td>0.761066</td>\n",
       "      <td>0.606347</td>\n",
       "      <td>0.697205</td>\n",
       "      <td>0.536440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.198300</td>\n",
       "      <td>0.207347</td>\n",
       "      <td>0.729098</td>\n",
       "      <td>0.585580</td>\n",
       "      <td>0.616095</td>\n",
       "      <td>0.557945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.184515</td>\n",
       "      <td>0.767623</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.580645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.174469</td>\n",
       "      <td>0.779098</td>\n",
       "      <td>0.650680</td>\n",
       "      <td>0.711048</td>\n",
       "      <td>0.599761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.175700</td>\n",
       "      <td>0.168646</td>\n",
       "      <td>0.789344</td>\n",
       "      <td>0.668387</td>\n",
       "      <td>0.726508</td>\n",
       "      <td>0.618877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.169100</td>\n",
       "      <td>0.167674</td>\n",
       "      <td>0.788115</td>\n",
       "      <td>0.677882</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.649940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.166300</td>\n",
       "      <td>0.160186</td>\n",
       "      <td>0.793033</td>\n",
       "      <td>0.675241</td>\n",
       "      <td>0.731198</td>\n",
       "      <td>0.627240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.299800</td>\n",
       "      <td>0.459877</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>0.510833</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.372600</td>\n",
       "      <td>0.459877</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>0.510833</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.328700</td>\n",
       "      <td>0.459877</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>0.510833</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.323500</td>\n",
       "      <td>0.459877</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>0.510833</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.309500</td>\n",
       "      <td>0.459877</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>0.510833</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>0.459877</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>0.510833</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.306700</td>\n",
       "      <td>0.459877</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>0.510833</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.291900</td>\n",
       "      <td>0.459877</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>0.510833</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.292600</td>\n",
       "      <td>0.459877</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>0.510833</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.279400</td>\n",
       "      <td>0.459877</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>0.510833</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.281400</td>\n",
       "      <td>0.459877</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>0.510833</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.459877</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>0.510833</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.268900</td>\n",
       "      <td>0.459877</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>0.510833</td>\n",
       "      <td>0.343033</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2061, training_loss=0.26238015205868237, metrics={'train_runtime': 1612.1979, 'train_samples_per_second': 40.862, 'train_steps_per_second': 1.278, 'total_flos': 1.7332655742517248e+16, 'train_loss': 0.26238015205868237, 'epoch': 3.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T02:58:28.032729Z",
     "start_time": "2025-01-08T02:58:27.288085Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.save_model(\"模型/文本相似模型\")",
   "id": "a319210522d750c8",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7、推理",
   "id": "285ef82381b8ff4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T02:58:28.048208Z",
     "start_time": "2025-01-08T02:58:28.042795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SentenceSimilarityPipeline:\n",
    "    def __init__(self, model_, tokenizer_):\n",
    "        self.model = model_.eval()\n",
    "        self.tokenizer = tokenizer_\n",
    "        self.device = model_.device\n",
    "\n",
    "    def __call__(self, text1, text2):\n",
    "        texts = []\n",
    "        if isinstance(text1, str) and isinstance(text2, str):\n",
    "            texts.extend([text1, text2])\n",
    "        elif isinstance(text1, str) and isinstance(text2, list):\n",
    "            for text_ in text2:\n",
    "                texts.extend([text1, text_])\n",
    "        elif isinstance(text1, list) and isinstance(text2, list):\n",
    "            assert len(text1) == len(text2), Exception(\"输入的长度要相同\")\n",
    "            for t1, t2 in zip(text1, text2):\n",
    "                texts.extend([t1, t2])\n",
    "        else:\n",
    "            raise Exception(\"输入的格式有问题\")\n",
    "\n",
    "        token = self.tokenizer(texts, max_length=256, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        token = {k: v.reshape(-1, 2, 256).to(self.device) for k, v in token.items()}\n",
    "        predict = self.model(**token)[1]\n",
    "        return predict.cpu().detach().numpy()"
   ],
   "id": "1ff1557f71cd268b",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T02:58:28.106260Z",
     "start_time": "2025-01-08T02:58:28.099452Z"
    }
   },
   "cell_type": "code",
   "source": "pipe = SentenceSimilarityPipeline(model, tokenizer)",
   "id": "220171ed990d76a7",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for text, a in dataset[\"test\"].to_pandas().groupby(\"text1\"):\n",
    "    predicts = pipe(text, a[\"text2\"].tolist())\n",
    "    print((predicts > 0.5).astype(int))\n",
    "    print(a[\"label\"].tolist())"
   ],
   "id": "38667654aacaa40b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
