{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T13:31:16.265220Z",
     "start_time": "2025-01-31T13:31:16.256752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ],
   "id": "4512983feca7ca90",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T13:31:21.875010Z",
     "start_time": "2025-01-31T13:31:16.380922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers.data import DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "id": "9ece3c74afdb7cfa",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# NER模型",
   "id": "ebc843d332043729"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T13:31:21.886057Z",
     "start_time": "2025-01-31T13:31:21.882037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label_list = \\\n",
    "    {0: 'O',\n",
    "     1: 'B-PER',\n",
    "     2: 'I-PER',\n",
    "     3: 'B-ORG',\n",
    "     4: 'I-ORG',\n",
    "     5: 'B-LOC',\n",
    "     6: 'I-LOC'}"
   ],
   "id": "3dc0c85394a05b3f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T13:31:21.943505Z",
     "start_time": "2025-01-31T13:31:21.903047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_parquet('数据/Token分类模型.parquet')\n",
    "print(df.shape)\n",
    "df.head()"
   ],
   "id": "38197fda2a3aa921",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [海, 钓, 比, 赛, 地, 点, 在, 厦, 门, 与, 金, 门, 之, 间, 的, ...   \n",
       "1  [这, 座, 依, 山, 傍, 水, 的, 博, 物, 馆, 由, 国, 内, 一, 流, ...   \n",
       "2  [但, 作, 为, 一, 个, 共, 产, 党, 员, 、, 人, 民, 公, 仆, ，, ...   \n",
       "3  [在, 发, 达, 国, 家, ，, 急, 救, 保, 险, 十, 分, 普, 及, ，, ...   \n",
       "4  [日, 俄, 两, 国, 国, 内, 政, 局, 都, 充, 满, 变, 数, ，, 尽, ...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[海, 钓, 比, 赛, 地, 点, 在, 厦, 门, 与, 金, 门, 之, 间, 的, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[这, 座, 依, 山, 傍, 水, 的, 博, 物, 馆, 由, 国, 内, 一, 流, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[但, 作, 为, 一, 个, 共, 产, 党, 员, 、, 人, 民, 公, 仆, ，, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[在, 发, 达, 国, 家, ，, 急, 救, 保, 险, 十, 分, 普, 及, ，, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[日, 俄, 两, 国, 国, 内, 政, 局, 都, 充, 满, 变, 数, ，, 尽, ...</td>\n",
       "      <td>[5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ner_data = Dataset.from_pandas(df).train_test_split(test_size=0.1)\n",
    "ner_data"
   ],
   "id": "91bdec83dbfc368f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1、数据处理",
   "id": "2f130339a1187967"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_path = 'hfl/chinese-macbert-base'",
   "id": "54c639ba57a44c67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer = AutoTokenizer.from_pretrained(model_path)",
   "id": "531df3d1d2f2f745",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_example(examples):\n",
    "    # is_split_into_words 意思是输入的单词是分割的\n",
    "    token = tokenizer(examples[\"tokens\"], is_split_into_words=True, truncation=True, max_length=512)\n",
    "    labels = []\n",
    "    for index, tags in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = token.word_ids(index)\n",
    "        label = [-100 if ids is None else tags[ids] for ids in word_ids]\n",
    "        labels.append(label)\n",
    "    token[\"labels\"] = labels\n",
    "    return token"
   ],
   "id": "67a73cd807e77551",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "net_dataset = ner_data.map(process_example, batched=True)",
   "id": "9bf47aad00083a51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2、创建模型",
   "id": "853f3f7882f52ec0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = AutoModelForTokenClassification.from_pretrained(model_path, num_labels=len(label_list), id2label=label_list)",
   "id": "9796d9a5efd5ea4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3、创建评估函数",
   "id": "26544f06491a7b33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n",
    "seqeval"
   ],
   "id": "d38f9f9ac9298629",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def metrics(values):\n",
    "    predict, labels = values\n",
    "    index = np.argmax(predict, axis=-1)\n",
    "\n",
    "    trues, predicates = [], []\n",
    "    for predicate, label in zip(index, labels):\n",
    "        true = [label_list[l] for l in label if l != -100]\n",
    "\n",
    "        predicate = [label_list[p] for p, l in zip(predicate, label) if l != -100]\n",
    "        trues.append(true)\n",
    "        predicates.append(predicate)\n",
    "    result = seqeval.compute(predictions=predicates, references=trues, mode=\"strict\", scheme=\"IOB2\")\n",
    "    return {\"f1\": result[\"overall_f1\"], \"recall\": result[\"overall_recall\"], \"precision\": result[\"overall_precision\"]}"
   ],
   "id": "53f79cc9a6b25e4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4、训练参数",
   "id": "86d6a29f973566a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:44:37.246286Z",
     "start_time": "2025-01-02T09:44:37.176852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"结果\",\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=100,\n",
    ")"
   ],
   "id": "68a5b3f521b7b7ea",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5、创建训练器",
   "id": "f4a0c5df6cdbf913"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T09:44:37.922226Z",
     "start_time": "2025-01-02T09:44:37.902061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=net_dataset[\"train\"],\n",
    "    eval_dataset=net_dataset[\"test\"],\n",
    "    compute_metrics=metrics,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer, max_length=512, padding=\"max_length\"),\n",
    ")"
   ],
   "id": "6d33e67b22ba1f7f",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T10:20:22.502070Z",
     "start_time": "2025-01-02T09:44:41.323190Z"
    }
   },
   "cell_type": "code",
   "source": "train.train()",
   "id": "84c430d7950dbf75",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lixiaoying/anaconda3/envs/AiMedJupyter/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2349' max='2349' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2349/2349 35:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.032274</td>\n",
       "      <td>0.929755</td>\n",
       "      <td>0.929441</td>\n",
       "      <td>0.930070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.033990</td>\n",
       "      <td>0.923060</td>\n",
       "      <td>0.935753</td>\n",
       "      <td>0.910706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.028721</td>\n",
       "      <td>0.931594</td>\n",
       "      <td>0.925609</td>\n",
       "      <td>0.937657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.030679</td>\n",
       "      <td>0.930164</td>\n",
       "      <td>0.921776</td>\n",
       "      <td>0.938705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.031788</td>\n",
       "      <td>0.938753</td>\n",
       "      <td>0.939811</td>\n",
       "      <td>0.937697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.027053</td>\n",
       "      <td>0.940223</td>\n",
       "      <td>0.941389</td>\n",
       "      <td>0.939060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.022372</td>\n",
       "      <td>0.946942</td>\n",
       "      <td>0.949504</td>\n",
       "      <td>0.944395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.023178</td>\n",
       "      <td>0.946931</td>\n",
       "      <td>0.949279</td>\n",
       "      <td>0.944594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.022117</td>\n",
       "      <td>0.942707</td>\n",
       "      <td>0.947701</td>\n",
       "      <td>0.937765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.023011</td>\n",
       "      <td>0.947085</td>\n",
       "      <td>0.950180</td>\n",
       "      <td>0.944009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.023487</td>\n",
       "      <td>0.946556</td>\n",
       "      <td>0.940261</td>\n",
       "      <td>0.952936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.021566</td>\n",
       "      <td>0.949747</td>\n",
       "      <td>0.952209</td>\n",
       "      <td>0.947298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.020679</td>\n",
       "      <td>0.952445</td>\n",
       "      <td>0.950406</td>\n",
       "      <td>0.954494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.021074</td>\n",
       "      <td>0.951340</td>\n",
       "      <td>0.951984</td>\n",
       "      <td>0.950698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.020540</td>\n",
       "      <td>0.951448</td>\n",
       "      <td>0.951984</td>\n",
       "      <td>0.950912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.020217</td>\n",
       "      <td>0.953978</td>\n",
       "      <td>0.955591</td>\n",
       "      <td>0.952370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.022157</td>\n",
       "      <td>0.956679</td>\n",
       "      <td>0.955816</td>\n",
       "      <td>0.957543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.022602</td>\n",
       "      <td>0.955628</td>\n",
       "      <td>0.954013</td>\n",
       "      <td>0.957249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.023299</td>\n",
       "      <td>0.955207</td>\n",
       "      <td>0.954238</td>\n",
       "      <td>0.956178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.022570</td>\n",
       "      <td>0.956718</td>\n",
       "      <td>0.956718</td>\n",
       "      <td>0.956718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.023153</td>\n",
       "      <td>0.955415</td>\n",
       "      <td>0.956492</td>\n",
       "      <td>0.954341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.023162</td>\n",
       "      <td>0.956180</td>\n",
       "      <td>0.959197</td>\n",
       "      <td>0.953181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.023024</td>\n",
       "      <td>0.956424</td>\n",
       "      <td>0.957394</td>\n",
       "      <td>0.955456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lixiaoying/anaconda3/envs/AiMedJupyter/lib/python3.10/site-packages/transformers/trainer.py:2254: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(best_model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2349, training_loss=0.010168542122018748, metrics={'train_runtime': 2140.7318, 'train_samples_per_second': 35.088, 'train_steps_per_second': 1.097, 'total_flos': 1.9627931820017664e+16, 'train_loss': 0.010168542122018748, 'epoch': 3.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6、预测",
   "id": "821ab1a19398dacc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T10:20:22.565283Z",
     "start_time": "2025-01-02T10:20:22.561357Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import pipeline",
   "id": "b407f89531013871",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T10:20:22.662578Z",
     "start_time": "2025-01-02T10:20:22.657490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# aggregation_strategy 指定聚合策略\n",
    "ner = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, device=0, aggregation_strategy=\"simple\")"
   ],
   "id": "8958028f5c191754",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T13:23:44.241626Z",
     "start_time": "2025-01-02T13:23:44.210401Z"
    }
   },
   "cell_type": "code",
   "source": "pd.DataFrame(ner(\"张伟明天去协和医院上班地址在北京\"))",
   "id": "973604c3ba81099e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  entity_group     score     word  start  end\n",
       "0          PER  0.999200      张 伟      0    2\n",
       "1          ORG  0.999289  协 和 医 院      5    9\n",
       "2          LOC  0.999236      北 京     14   16"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.999200</td>\n",
       "      <td>张 伟</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.999289</td>\n",
       "      <td>协 和 医 院</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.999236</td>\n",
       "      <td>北 京</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
